{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7edb8279-02b6-4f1a-a306-fa4d2ff11a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType, ArrayType, IntegerType, BooleanType\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier,  LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.functions import vector_to_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54fbb9b6-ff2c-433c-afe5-75c7a31e5609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Task 1 - Video game sales data\n",
    "\n",
    "\n",
    "Load the data from the CSV file into a data frame. The column headers and the first few data lines should give sufficient information about the source dataset. The numbers in the sales columns are given in millions.\n",
    "\n",
    "Using the data, find answers to the following:\n",
    "\n",
    "- Which publisher has the highest total sales in video games in North America considering games released in years 2006-2015?\n",
    "- How many titles in total for this publisher do not have sales data available for North America considering games released in years 2006-2015?\n",
    "- Separating games released in different years and considering only this publisher and only games released in years 2006-2015, what are the total sales, in North America and globally, for each year?\n",
    "    - I.e., what are the total sales (in North America and globally) for games released by this publisher in year 2006? And the same for year 2007? ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32c74fba-71de-4818-bb35-6fa586b7cd7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "videogameDF: DataFrame = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"abfss://shared@tunics320f2024gen2.dfs.core.windows.net/assignment/sales/video_game_sales.csv\") \\\n",
    "    .select(\"Publisher\", \"release_date\", \"total_sales\", \"na_sales\")\n",
    "\n",
    "\n",
    "videogameDF = videogameDF.withColumn(\"Year\", F.year(F.col(\"release_date\"))) \\\n",
    "    .filter((F.col(\"Year\") >= 2006) & (F.col(\"Year\") <= 2015))\n",
    "\n",
    "videogameDF = videogameDF.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da850da0-73b5-4b4d-be43-7d5f70448d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The publisher with the highest total video game sales in North America is: 'Activision'\nThe number of titles with missing sales data for North America: 230\nSales data for the publisher:\n+----+--------+------------+\n|Year|na_total|global_total|\n+----+--------+------------+\n|2006|   14.55|       19.99|\n|2007|    26.9|       42.11|\n|2008|   39.21|       63.38|\n|2009|   45.08|       74.95|\n|2010|   37.92|       60.08|\n|2011|   28.63|       51.29|\n|2012|   23.08|        46.0|\n|2013|   20.92|       39.64|\n|2014|   21.51|       42.45|\n|2015|   19.67|       38.98|\n+----+--------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "NAsaleswithpublisherDF = videogameDF.groupBy(\"Publisher\") \\\n",
    "    .agg(F.sum(\"na_sales\").alias(\"na_sales\")) \\\n",
    "    .orderBy(F.col(\"na_sales\").desc())\n",
    "\n",
    "bestNAPublisher: str = NAsaleswithpublisherDF.first()[\"Publisher\"]\n",
    "\n",
    "bestNApublishergames = videogameDF.filter(F.col(\"Publisher\") == bestNAPublisher)\n",
    "\n",
    "titlesWithMissingSalesData: int = bestNApublishergames.filter(F.isnull(F.col(\"NA_Sales\"))).count()\n",
    "\n",
    "bestNAPublisherSales: DataFrame = bestNApublishergames.groupBy(\"Year\") \\\n",
    "    .agg(\n",
    "        F.round(F.sum(\"na_Sales\"),2).alias(\"na_total\"),\n",
    "        F.round(F.sum(\"total_sales\"),2).alias(\"global_total\")\n",
    "    ) \\\n",
    "    .orderBy(\"Year\")\n",
    "\n",
    "print(f\"The publisher with the highest total video game sales in North America is: '{bestNAPublisher}'\")\n",
    "print(f\"The number of titles with missing sales data for North America: {titlesWithMissingSalesData}\")\n",
    "print(\"Sales data for the publisher:\")\n",
    "bestNAPublisherSales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6b6768-e456-4dbb-8633-95ce021a73c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Task 2 - Event data from football matches\n",
    "\n",
    "#### Background information\n",
    "\n",
    "In the considered leagues, a season is played in a double round-robin format where each team plays against all other teams twice. Once as a home team in their own stadium and once as an away team in the other team's stadium. A season usually starts in August and ends in May.\n",
    "\n",
    "Each league match consists of two halves of 45 minutes each. Each half runs continuously, meaning that the clock is not stopped when the ball is out of play. The referee of the match may add some additional time to each half based on game stoppages. \\[[https://en.wikipedia.org/wiki/Association_football#90-minute_ordinary_time](https://en.wikipedia.org/wiki/Association_football#90-minute_ordinary_time)\\]\n",
    "\n",
    "The team that scores more goals than their opponent wins the match.\n",
    "\n",
    "**Columns in the data**\n",
    "\n",
    "Each row in the given data represents an event in a specific match. An event can be, for example, a pass, a foul, a shot, or a save attempt.\n",
    "\n",
    "Simple explanations for the available columns. Not all of these will be needed in this assignment.\n",
    "\n",
    "| column name | column type | description |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| competition | string | The name of the competition |\n",
    "| season | string | The season the match was played |\n",
    "| matchId | integer | A unique id for the match |\n",
    "| eventId | integer | A unique id for the event |\n",
    "| homeTeam | string | The name of the home team |\n",
    "| awayTeam | string | The name of the away team |\n",
    "| event | string | The main category for the event |\n",
    "| subEvent | string | The subcategory for the event |\n",
    "| eventTeam | string | The name of the team that initiated the event |\n",
    "| eventPlayerId | integer | The id for the player who initiated the event |\n",
    "| eventPeriod | string | `1H` for events in the first half, `2H` for events in the second half |\n",
    "| eventTime | double | The event time in seconds counted from the start of the half |\n",
    "| tags | array of strings | The descriptions of the tags associated with the event |\n",
    "| startPosition | struct | The event start position given in `x` and `y` coordinates in range \\[0,100\\] |\n",
    "| enPosition | struct | The event end position given in `x` and `y` coordinates in range \\[0,100\\] |\n",
    "\n",
    "The used event categories can be seen from `assignment/football/metadata/eventid2name.csv`.<br>\n",
    "And all available tag descriptions from `assignment/football/metadata/tags2name.csv`.<br>\n",
    "You don't need to access these files in the assignment, but they can provide context for the following basic tasks that will use the event data.\n",
    "\n",
    "#### The task\n",
    "\n",
    "In this task you should load the data with all the rows into a data frame. This data frame object will then be used in the following basic tasks 3-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bf17976-0c4a-460e-bb1c-f70f2f5cbe14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eventDF: DataFrame = spark \\\n",
    "    .read \\\n",
    "    .parquet(\"abfss://shared@tunics320f2024gen2.dfs.core.windows.net/assignment/football/events.parquet\") \\\n",
    "    .select(\"competition\", \"season\", \"matchId\",  \"homeTeam\", \"awayTeam\",\"event\", \"eventTeam\", \"tags\").cache() #since this is used again "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a39f1fc-9304-495d-b9bc-75cff895f654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Task 3 - Calculate match results\n",
    "\n",
    "Create a match data frame for all the matches included in the event data frame created in basic task 2.\n",
    "\n",
    "The resulting data frame should contain one row for each match and include the following columns:\n",
    "\n",
    "| column name   | column type | description |\n",
    "| ------------- | ----------- | ----------- |\n",
    "| matchId       | integer     | A unique id for the match |\n",
    "| competition   | string      | The name of the competition |\n",
    "| season        | string      | The season the match was played |\n",
    "| homeTeam      | string      | The name of the home team |\n",
    "| awayTeam      | string      | The name of the away team |\n",
    "| homeTeamGoals | integer     | The number of goals scored by the home team |\n",
    "| awayTeamGoals | integer     | The number of goals scored by the away team |\n",
    "\n",
    "The number of goals scored for each team should be determined by the available event data.<br>\n",
    "There are two events related to each goal:\n",
    "\n",
    "- One event for the player that scored the goal. This includes possible own goals.\n",
    "- One event for the goalkeeper that tried to stop the goal.\n",
    "\n",
    "You need to choose which types of events you are counting.<br>\n",
    "If you count both of the event types mentioned above, you will get double the amount of actual goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a04180-04c2-4a07-a501-4833374f8152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "goalEventsDF = eventDF.filter(\n",
    "    F.array_contains(F.col(\"tags\"), \"Goal\") & (F.col(\"event\") == \"Save attempt\") \n",
    ")\n",
    "\n",
    "goalsPerMatchDF = goalEventsDF.groupBy(\"matchId\", \"eventTeam\", \"homeTeam\", \"awayTeam\") \\\n",
    "    .agg(F.count(\"*\").alias(\"totalGoals\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b06b2b12-3075-4abc-8038-42e4fe804280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "matchGoalsAggregatedDF = goalsPerMatchDF.groupBy(\"matchId\") \\\n",
    "    .agg(\n",
    "        F.sum(F.when(F.col(\"eventTeam\") == F.col(\"awayTeam\"), F.col(\"totalGoals\")).otherwise(0)).cast(\"int\").alias(\"homeTeamGoals\"),\n",
    "        F.sum(F.when(F.col(\"eventTeam\") == F.col(\"homeTeam\"), F.col(\"totalGoals\")).otherwise(0)).cast(\"int\").alias(\"awayTeamGoals\")\n",
    "    )\n",
    "\n",
    "\n",
    "matchDF = eventDF.select(\"matchId\", \"competition\", \"season\", \"homeTeam\", \"awayTeam\").distinct().join(matchGoalsAggregatedDF, \"matchId\", \"left\")\n",
    "\n",
    "matchDF = matchDF.fillna({\"homeTeamGoals\": 0, \"awayTeamGoals\": 0}).cache() #Since this is reused\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a14e010a-b9dc-45a0-9a52-2f2d92ff6e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Task 4 - Calculate team points in a season\n",
    "\n",
    "Create a season data frame that uses the match data frame from the basic task 3 and contains aggregated seasonal results and statistics for all the teams in all leagues. While the used dataset only includes data from a single season for each league, the code should be written such that it would work even if the data would include matches from multiple seasons for each league.\n",
    "\n",
    "###### Game result determination\n",
    "\n",
    "- Team wins the match if they score more goals than their opponent.\n",
    "- The match is considered a draw if both teams score equal amount of goals.\n",
    "- Team loses the match if they score fewer goals than their opponent.\n",
    "\n",
    "###### Match point determination\n",
    "\n",
    "- The winning team gains 3 points from the match.\n",
    "- Both teams gain 1 point from a drawn match.\n",
    "- The losing team does not gain any points from the match.\n",
    "\n",
    "The resulting data frame should contain one row for each team per league and season. It should include the following columns:\n",
    "\n",
    "| column name    | column type | description |\n",
    "| -------------- | ----------- | ----------- |\n",
    "| competition    | string      | The name of the competition |\n",
    "| season         | string      | The season |\n",
    "| team           | string      | The name of the team |\n",
    "| games          | integer     | The number of games the team played in the given season |\n",
    "| wins           | integer     | The number of wins the team had in the given season |\n",
    "| draws          | integer     | The number of draws the team had in the given season |\n",
    "| losses         | integer     | The number of losses the team had in the given season |\n",
    "| goalsScored    | integer     | The total number of goals the team scored in the given season |\n",
    "| goalsConceded  | integer     | The total number of goals scored against the team in the given season |\n",
    "| points         | integer     | The total number of points gained by the team in the given season |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4084f76-7fb8-4e6c-aa25-7bc5de61b739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "teamResultsDF = matchDF.select(\n",
    "    F.col(\"competition\"),\n",
    "    F.col(\"season\"),\n",
    "    F.col(\"homeTeam\").alias(\"team\"),\n",
    "    F.col(\"awayTeam\").alias(\"opponent\"),\n",
    "    F.col(\"homeTeamGoals\").alias(\"goalsScored\"),\n",
    "    F.col(\"awayTeamGoals\").alias(\"goalsConceded\"),\n",
    "    F.when(F.col(\"homeTeamGoals\") > F.col(\"awayTeamGoals\"), 3)  # Win\n",
    "     .when(F.col(\"homeTeamGoals\") == F.col(\"awayTeamGoals\"), 1)  # Draw\n",
    "     .otherwise(0).alias(\"points\"),\n",
    "    F.when(F.col(\"homeTeamGoals\") > F.col(\"awayTeamGoals\"), 1)  # Win\n",
    "     .otherwise(0).alias(\"wins\"),\n",
    "    F.when(F.col(\"homeTeamGoals\") == F.col(\"awayTeamGoals\"), 1)  # Draw\n",
    "     .otherwise(0).alias(\"draws\"),\n",
    "    F.when(F.col(\"homeTeamGoals\") < F.col(\"awayTeamGoals\"), 1)  # Loss\n",
    "     .otherwise(0).alias(\"losses\")\n",
    ").union(\n",
    "    matchDF.select(\n",
    "        F.col(\"competition\"),\n",
    "        F.col(\"season\"),\n",
    "        F.col(\"awayTeam\").alias(\"team\"),\n",
    "        F.col(\"homeTeam\").alias(\"opponent\"),\n",
    "        F.col(\"awayTeamGoals\").alias(\"goalsScored\"),\n",
    "        F.col(\"homeTeamGoals\").alias(\"goalsConceded\"),\n",
    "        F.when(F.col(\"awayTeamGoals\") > F.col(\"homeTeamGoals\"), 3)  # Win\n",
    "         .when(F.col(\"awayTeamGoals\") == F.col(\"homeTeamGoals\"), 1)  # Draw\n",
    "         .otherwise(0).alias(\"points\"),\n",
    "        F.when(F.col(\"awayTeamGoals\") > F.col(\"homeTeamGoals\"), 1)  # Win\n",
    "         .otherwise(0).alias(\"wins\"),\n",
    "        F.when(F.col(\"awayTeamGoals\") == F.col(\"homeTeamGoals\"), 1)  # Draw\n",
    "         .otherwise(0).alias(\"draws\"),\n",
    "        F.when(F.col(\"awayTeamGoals\") < F.col(\"homeTeamGoals\"), 1)  # Loss\n",
    "         .otherwise(0).alias(\"losses\")\n",
    "    )\n",
    ")\n",
    "\n",
    "seasonDF = teamResultsDF.groupBy(\"competition\", \"season\", \"team\").agg(\n",
    "    F.count(\"*\").cast(\"int\").alias(\"games\"),\n",
    "    F.sum(\"wins\").cast(\"int\").alias(\"wins\"),\n",
    "    F.sum(\"draws\").cast(\"int\").alias(\"draws\"),\n",
    "    F.sum(\"losses\").cast(\"int\").alias(\"losses\"),\n",
    "    F.sum(\"goalsScored\").cast(\"int\").alias(\"goalsScored\"),\n",
    "    F.sum(\"goalsConceded\").cast(\"int\").alias(\"goalsConceded\"),\n",
    "    F.sum(\"points\").cast(\"int\").alias(\"points\")\n",
    ").cache() #Since reuse this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0425c0cd-e7a9-42e3-8dfd-2462de6969ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Task 5 - English Premier League table\n",
    "\n",
    "Using the season data frame from basic task 4 calculate the final league table for `English Premier League` in season `2017-2018`.\n",
    "\n",
    "The result should be given as data frame which is ordered by the team's classification for the season.\n",
    "\n",
    "A team is classified higher than the other team if one of the following is true:\n",
    "\n",
    "- The team has a higher number of total points than the other team\n",
    "- The team has an equal number of points, but have a better goal difference than the other team\n",
    "- The team has an equal number of points and goal difference, but have more goals scored in total than the other team\n",
    "\n",
    "Goal difference is the difference between the number of goals scored for and against the team.\n",
    "\n",
    "The resulting data frame should contain one row for each team.<br>\n",
    "It should include the following columns (several columns renamed trying to match the [league table in Wikipedia](https://en.wikipedia.org/wiki/2017%E2%80%9318_Premier_League#League_table)):\n",
    "\n",
    "| column name | column type | description |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| Pos         | integer     | The classification of the team |\n",
    "| Team        | string      | The name of the team |\n",
    "| Pld         | integer     | The number of games played |\n",
    "| W           | integer     | The number of wins |\n",
    "| D           | integer     | The number of draws |\n",
    "| L           | integer     | The number of losses |\n",
    "| GF          | integer     | The total number of goals scored by the team |\n",
    "| GA          | integer     | The total number of goals scored against the team |\n",
    "| GD          | string      | The goal difference |\n",
    "| Pts         | integer     | The total number of points gained by the team |\n",
    "\n",
    "The goal difference should be given as a string with an added `+` at the beginning if the difference is positive, similarly to the table in the linked Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0b15b3-0f07-441c-8abf-dce343c4e967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Premier League table for season 2017-2018\n+---+----------------------+---+---+---+---+---+---+---+---+\n|Pos|Team                  |Pld|W  |D  |L  |GF |GA |GD |Pts|\n+---+----------------------+---+---+---+---+---+---+---+---+\n|1  |Manchester City       |38 |32 |4  |2  |106|27 |+79|100|\n|2  |Manchester United     |38 |25 |6  |7  |68 |28 |+40|81 |\n|3  |Tottenham Hotspur     |38 |23 |8  |7  |74 |36 |+38|77 |\n|4  |Liverpool             |38 |21 |12 |5  |84 |38 |+46|75 |\n|5  |Chelsea               |38 |21 |7  |10 |62 |38 |+24|70 |\n|6  |Arsenal               |38 |19 |6  |13 |74 |51 |+23|63 |\n|7  |Burnley               |38 |14 |12 |12 |36 |39 |-3 |54 |\n|8  |Everton               |38 |13 |10 |15 |44 |58 |-14|49 |\n|9  |Leicester City        |38 |12 |11 |15 |56 |60 |-4 |47 |\n|10 |Newcastle United      |38 |12 |8  |18 |39 |47 |-8 |44 |\n|11 |Crystal Palace        |38 |11 |11 |16 |45 |55 |-10|44 |\n|12 |AFC Bournemouth       |38 |11 |11 |16 |45 |61 |-16|44 |\n|13 |West Ham United       |38 |10 |12 |16 |48 |68 |-20|42 |\n|14 |Watford               |38 |11 |8  |19 |44 |64 |-20|41 |\n|15 |Brighton & Hove Albion|38 |9  |13 |16 |34 |54 |-20|40 |\n|16 |Huddersfield Town     |38 |9  |10 |19 |28 |58 |-30|37 |\n|17 |Southampton           |38 |7  |15 |16 |37 |56 |-19|36 |\n|18 |Swansea City          |38 |8  |9  |21 |28 |56 |-28|33 |\n|19 |Stoke City            |38 |7  |12 |19 |35 |68 |-33|33 |\n|20 |West Bromwich Albion  |38 |6  |13 |19 |31 |56 |-25|31 |\n+---+----------------------+---+---+---+---+---+---+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "englandDF: DataFrame = seasonDF.filter(\n",
    "    (F.col(\"competition\") == \"English Premier League\") & (F.col(\"season\") == \"2017-2018\")\n",
    ").withColumn(\n",
    "    \"GD\",\n",
    "    F.when((F.col(\"goalsScored\") - F.col(\"goalsConceded\")) > 0,\n",
    "           F.concat(F.lit(\"+\"), (F.col(\"goalsScored\") - F.col(\"goalsConceded\")).cast(\"string\"))\n",
    "    ).otherwise((F.col(\"goalsScored\") - F.col(\"goalsConceded\")).cast(\"string\"))\n",
    ")\n",
    "\n",
    "englandDF = englandDF.select(\n",
    "    F.col(\"team\").alias(\"Team\"),\n",
    "    F.col(\"games\").alias(\"Pld\"),\n",
    "    F.col(\"wins\").alias(\"W\"),\n",
    "    F.col(\"draws\").alias(\"D\"),\n",
    "    F.col(\"losses\").alias(\"L\"),\n",
    "    F.col(\"goalsScored\").alias(\"GF\"),\n",
    "    F.col(\"goalsConceded\").alias(\"GA\"),\n",
    "    F.col(\"GD\"),\n",
    "    F.col(\"points\").alias(\"Pts\")\n",
    ")\n",
    "\n",
    "windowSpec = Window.orderBy(\n",
    "    F.col(\"Pts\").desc(),\n",
    "    (F.col(\"GF\") - F.col(\"GA\")).desc(),\n",
    "    F.col(\"GF\").desc()\n",
    ")\n",
    "\n",
    "englandDF = englandDF.withColumn(\"Pos\", F.row_number().over(windowSpec))\n",
    "\n",
    "# Reorder columns\n",
    "englandDF = englandDF.select(\n",
    "    \"Pos\", \"Team\", \"Pld\", \"W\", \"D\", \"L\", \"GF\", \"GA\", \"GD\", \"Pts\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"English Premier League table for season 2017-2018\")\n",
    "englandDF.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "413a7bdd-f89c-454f-b5b5-8b947fe6b70d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic task 6: Calculate the number of passes\n",
    "\n",
    "This task involves going back to the event data frame and counting the number of passes each team made in each match. A pass is considered successful if it is marked as `Accurate`.\n",
    "\n",
    "Using the event data frame from basic task 2, calculate the total number of passes as well as the total number of successful passes for each team in each match.<br>\n",
    "The resulting data frame should contain one row for each team in each match, i.e., two rows for each match. It should include the following columns:\n",
    "\n",
    "| column name | column type | description |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| matchId     | integer     | A unique id for the match |\n",
    "| competition | string      | The name of the competition |\n",
    "| season      | string      | The season |\n",
    "| team        | string      | The name of the team |\n",
    "| totalPasses | integer     | The total number of passes the team attempted in the match |\n",
    "| successfulPasses | integer | The total number of successful passes made by the team in the match |\n",
    "\n",
    "You can assume that each team had at least one pass attempt in each match they played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05bbd448-9b18-4e3a-a2af-ece1e1250d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "matchPassDF: DataFrame = eventDF.filter(F.col(\"event\") == \"Pass\").select(\n",
    "    \"matchId\", \"eventTeam\", \"competition\", \"season\", \n",
    "    F.when(F.array_contains(F.col(\"tags\"), \"Accurate\"), 1).otherwise(0).alias(\"isAccurate\")\n",
    ")\n",
    "\n",
    "matchPassDF = matchPassDF.groupBy(\n",
    "    \"matchId\", \"eventTeam\",\"competition\", \"season\"\n",
    ").agg(                  \n",
    "    F.sum(\"isAccurate\").cast(\"int\").alias(\"successfulPasses\") ,\n",
    "    F.count(\"*\").cast(\"int\").alias(\"totalPasses\")        \n",
    ")\n",
    "\n",
    "matchPassDF = matchPassDF.withColumnRenamed(\"eventTeam\", \"team\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7677001-1354-4572-b1fa-60e74b1eb442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Task 7: Teams with the worst passes\n",
    "\n",
    "Using the match pass data frame from basic task 6 find the teams with the lowest average ratio for successful passes over the season `2017-2018` for each league.\n",
    "\n",
    "The ratio for successful passes over a single match is the number of successful passes divided by the number of total passes.<br>\n",
    "The average ratio over the season is the average of the single match ratios.\n",
    "\n",
    "Give the result as a data frame that has one row for each league-team pair with the following columns:\n",
    "\n",
    "| column name | column type | description |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| competition | string      | The name of the competition |\n",
    "| team        | string      | The name of the team |\n",
    "| passSuccessRatio | double | The average ratio for successful passes over the season given as percentages rounded to two decimals |\n",
    "\n",
    "Order the data frame so that the team with the lowest ratio for passes is given first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "537d1371-9ad5-4446-949b-3fead3a995f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The teams with the lowest ratios for successful passes for each league in season 2017-2018:\n+----------------------+----------+----------------+\n|competition           |team      |passSuccessRatio|\n+----------------------+----------+----------------+\n|Spanish La Liga       |Getafe    |72.37           |\n|Italian Serie A       |Crotone   |74.74           |\n|English Premier League|Stoke City|76.28           |\n|German Bundesliga     |Augsburg  |76.44           |\n|French Ligue 1        |Toulouse  |77.51           |\n+----------------------+----------+----------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "matchPassDF = matchPassDF.filter(F.col(\"season\") == \"2017-2018\").withColumn(\n",
    "    \"passSuccessRatio\", (F.col(\"successfulPasses\") / F.col(\"totalPasses\") * 100).cast(\"double\")\n",
    ")\n",
    "\n",
    "\n",
    "windowSpec = Window.partitionBy(\"competition\").orderBy(\"passSuccessRatio\")\n",
    "lowestPassSuccessRatioDF = (\n",
    "    matchPassDF.groupBy(\"competition\", \"team\")\n",
    "    .agg(F.round(F.avg(\"passSuccessRatio\"), 2).alias(\"passSuccessRatio\"))\n",
    "    .withColumn(\"rank\", F.row_number().over(windowSpec))\n",
    "    .orderBy(\"rank\", \"passSuccessRatio\")\n",
    "    .drop(\"rank\")\n",
    ").cache() #Since this is used in task 8\n",
    "\n",
    "\n",
    "print(\"The teams with the lowest ratios for successful passes for each league in season 2017-2018:\")\n",
    "lowestPassSuccessRatioDF.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ff5eab-08c1-4c2a-bb42-2bc080d7e784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic task 8: The best teams\n",
    "\n",
    "For this task the best teams are determined by having the highest point average per match.\n",
    "\n",
    "Using the data frames created in the previous tasks find the two best teams from each league in season `2017-2018` with their full statistics.\n",
    "\n",
    "Give the result as a data frame with the following columns:\n",
    "\n",
    "| column name | column type | description |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| Team        | string      | The name of the team |\n",
    "| League      | string      | The name of the league |\n",
    "| Pos         | integer     | The classification of the team within their league |\n",
    "| Pld         | integer     | The number of games played |\n",
    "| W           | integer     | The number of wins |\n",
    "| D           | integer     | The number of draws |\n",
    "| L           | integer     | The number of losses |\n",
    "| GF          | integer     | The total number of goals scored by the team |\n",
    "| GA          | integer     | The total number of goals scored against the team |\n",
    "| GD          | string      | The goal difference |\n",
    "| Pts         | integer     | The total number of points gained by the team |\n",
    "| Avg         | double      | The average points per match gained by the team |\n",
    "| PassRatio   | double      | The average ratio for successful passes over the season given as percentages rounded to two decimals |\n",
    "\n",
    "Order the data frame so that the team with the highest point average per match is given first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871a8065-411d-4728-83e5-eff7eda19522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 2 teams for each league in season 2017-2018\n+-----------------+----------------------+---+---+---+---+---+---+---+---+---+----+---------+\n|Team             |League                |Pos|Pld|W  |D  |L  |GF |GA |GD |Pts|Avg |PassRatio|\n+-----------------+----------------------+---+---+---+---+---+---+---+---+---+----+---------+\n|Manchester City  |English Premier League|1  |38 |32 |4  |2  |106|27 |+79|100|2.63|89.62    |\n|Juventus         |Italian Serie A       |1  |38 |30 |5  |3  |86 |24 |+62|95 |2.5 |87.96    |\n|Bayern München   |German Bundesliga     |1  |34 |27 |3  |4  |92 |28 |+64|84 |2.47|87.83    |\n|PSG              |French Ligue 1        |1  |38 |29 |6  |3  |108|29 |+79|93 |2.45|89.16    |\n|Barcelona        |Spanish La Liga       |1  |38 |28 |9  |1  |99 |29 |+70|93 |2.45|88.35    |\n|Napoli           |Italian Serie A       |2  |38 |28 |7  |3  |77 |29 |+48|91 |2.39|87.87    |\n|Manchester United|English Premier League|2  |38 |25 |6  |7  |68 |28 |+40|81 |2.13|84.79    |\n|Monaco           |French Ligue 1        |2  |38 |24 |8  |6  |85 |45 |+40|80 |2.11|82.52    |\n|Atlético Madrid  |Spanish La Liga       |2  |38 |23 |10 |5  |58 |22 |+36|79 |2.08|82.51    |\n|Schalke 04       |German Bundesliga     |2  |34 |18 |9  |7  |53 |37 |+16|63 |1.85|81.96    |\n+-----------------+----------------------+---+---+---+---+---+---+---+---+---+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "seasonDF = seasonDF.withColumn(\n",
    "    \"Avg\", F.col(\"points\") / F.col(\"games\")\n",
    ")\n",
    "\n",
    "rankedTeamDF = seasonDF.join(lowestPassSuccessRatioDF, [\"competition\", \"team\"], \"inner\")\n",
    "\n",
    "windowSpec = Window.partitionBy(\"competition\").orderBy(F.col(\"Avg\").desc())\n",
    "rankedTeamDF = rankedTeamDF.withColumn(\"rank\", F.rank().over(windowSpec))\n",
    "\n",
    "topTeamsDF = rankedTeamDF.filter(F.col(\"rank\") <= 2)\n",
    "\n",
    "bestDF: DataFrame = topTeamsDF.select(\n",
    "    F.col(\"team\").alias(\"Team\"),\n",
    "    F.col(\"competition\").alias(\"League\"),\n",
    "    F.col(\"rank\").alias(\"Pos\"),\n",
    "    F.col(\"games\").alias(\"Pld\"),\n",
    "    F.col(\"wins\").alias(\"W\"),\n",
    "    F.col(\"draws\").alias(\"D\"),\n",
    "    F.col(\"losses\").alias(\"L\"),\n",
    "    F.col(\"goalsScored\").alias(\"GF\"),\n",
    "    F.col(\"goalsConceded\").alias(\"GA\"),\n",
    "    F.when(\n",
    "        (F.col(\"goalsScored\") - F.col(\"goalsConceded\")) > 0,\n",
    "        F.concat(F.lit(\"+\"), (F.col(\"goalsScored\") - F.col(\"goalsConceded\")).cast(\"string\"))\n",
    "    ).otherwise((F.col(\"goalsScored\") - F.col(\"goalsConceded\")).cast(\"string\")).alias(\"GD\"),\n",
    "    F.col(\"points\").alias(\"Pts\"),\n",
    "    F.round(F.col(\"Avg\"), 2).alias(\"Avg\"),\n",
    "    F.round(F.col(\"passSuccessRatio\"), 2).alias(\"PassRatio\")\n",
    ").orderBy(F.col(\"Avg\").desc())\n",
    "\n",
    "\n",
    "print(\"The top 2 teams for each league in season 2017-2018\")\n",
    "bestDF.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb35ba1-7b7e-4b7c-b5fa-7594cac63d91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Advanced Task 3 - Image data and pixel colors (2 points)\n",
    "\n",
    "This advanced task involves loading in PNG image data and complementing JSON metadata into Spark data structure. And then determining the colors of the pixels in the images, and finding the answers to several color related questions.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "The target of the task is to combine the image data with the JSON data, determine the image pixel colors, and the find the answers to the following questions:\n",
    "\n",
    "- Which four images have the most colored non-transparent pixels?\n",
    "- Which five images have the lowest ratio of colored vs. transparent pixels?\n",
    "- What are the three most common colors in the Finnish flag image (annotation: `flag: Finland`)?\n",
    "    - And how many percentages of the colored pixels does each color have?\n",
    "- How many images have their most common three colors as, `Blue`-`Yellow`-`Black`, in that order?\n",
    "- Which five images have the most red pixels among the image group `activities`?\n",
    "    - And how many red pixels do each of these images have?\n",
    "\n",
    "It might be advisable to test your work-in-progress code with a limited number of images before using the full image set.<br>\n",
    "You are free to choose your own approach to the task: user defined functions with data frames, RDDs/Datasets, or combination of both.\n",
    "\n",
    "Note that the currently the Python helper functions do not exactly match the Scala versions, and thus the answers to the questions might not quite match the given example results in the example output notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c729d5d3-aab3-43dc-93f2-8bba93712b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "imageDF = spark.read.format(\"image\").load(\"abfss://shared@tunics320f2024gen2.dfs.core.windows.net/assignment/openmoji/color\")\n",
    "jsonDF = spark.read.json(\"abfss://shared@tunics320f2024gen2.dfs.core.windows.net/assignment/openmoji/metadata/openmoji.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3883852f-aff0-4475-9c7f-7f511d824c99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# separates binary image data to an array of hex strings that represent the pixels\n",
    "# assumes 8-bit representation for each pixel (0x00 - 0xff)\n",
    "# with `channels` attribute representing how many bytes is used for each pixel\n",
    "def toPixels(data: bytes, channels: int) -> List[str]:\n",
    "    return [\n",
    "        \"\".join([\n",
    "            f\"{data[index+byte]:02X}\"\n",
    "            for byte in range(0, channels)\n",
    "        ])\n",
    "        for index in range(0, len(data), channels)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81e70df-05d4-4169-8e88-a9da102e7529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "   \n",
    "toPixels_udf = F.udf(toPixels, ArrayType(StringType()))\n",
    "\n",
    "imageDF = imageDF.withColumn(\"hex_pixels\", toPixels_udf(F.col(\"image.data\"), F.col(\"image.nChannels\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc7eedd7-b751-4f65-9d4b-849ee903837f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# naive implementation of picking the name of the pixel color based on the input hex representation of the pixel\n",
    "# only works for OpenCV type CV_8U (mode=24) compatible input\n",
    "def toColorName(hexString: str) -> str:\n",
    "    # mapping of RGB values to basic color names\n",
    "    colors: Dict[Tuple[int, int, int], str] = {\n",
    "        (0, 0, 0):     \"Black\",  (0, 0, 128):     \"Blue\",   (0, 0, 255):     \"Blue\",\n",
    "        (0, 128, 0):   \"Green\",  (0, 128, 128):   \"Green\",  (0, 128, 255):   \"Blue\",\n",
    "        (0, 255, 0):   \"Green\",  (0, 255, 128):   \"Green\",  (0, 255, 255):   \"Blue\",\n",
    "        (128, 0, 0):   \"Red\",    (128, 0, 128):   \"Purple\", (128, 0, 255):   \"Purple\",\n",
    "        (128, 128, 0): \"Green\",  (128, 128, 128): \"Gray\",   (128, 128, 255): \"Purple\",\n",
    "        (128, 255, 0): \"Green\",  (128, 255, 128): \"Green\",  (128, 255, 255): \"Blue\",\n",
    "        (255, 0, 0):   \"Red\",    (255, 0, 128):   \"Pink\",   (255, 0, 255):   \"Purple\",\n",
    "        (255, 128, 0): \"Orange\", (255, 128, 128): \"Orange\", (255, 128, 255): \"Pink\",\n",
    "        (255, 255, 0): \"Yellow\", (255, 255, 128): \"Yellow\", (255, 255, 255): \"White\"\n",
    "    }\n",
    "\n",
    "    # helper function to round values of 0-255 to the nearest of 0, 128, or 255\n",
    "    def roundColorValue(value: int) -> int:\n",
    "        if value < 85:\n",
    "            return 0\n",
    "        if value < 170:\n",
    "            return 128\n",
    "        return 255\n",
    "\n",
    "    validString: bool = re.match(r\"[0-9a-fA-F]{8}\", hexString) is not None\n",
    "    if validString:\n",
    "        # for OpenCV type CV_8U (mode=24) the expected order of bytes is BGRA\n",
    "        blue: int = roundColorValue(int(hexString[0:2], 16))\n",
    "        green: int = roundColorValue(int(hexString[2:4], 16))\n",
    "        red: int = roundColorValue(int(hexString[4:6], 16))\n",
    "        alpha: int = int(hexString[6:8], 16)\n",
    "\n",
    "        if alpha < 128:\n",
    "            return \"None\"  # any pixel with less than 50% opacity is considered as color \"None\"\n",
    "        return colors[(red, green, blue)]\n",
    "\n",
    "    return \"None\"  # any input that is not in valid format is considered as color \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a65ec2cb-fd00-4278-9d09-ec65b97a4ac0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@udf(ArrayType(StringType()))\n",
    "def toColorNamesudf(hex_pixels):\n",
    "    return [toColorName(pixel) for pixel in hex_pixels]\n",
    "\n",
    "# Add a new column with the color names for each pixel\n",
    "imageDF = imageDF.withColumn(\"color_names\", toColorNamesudf(F.col(\"hex_pixels\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9b6950-dae0-4b51-b5a8-5de66216b792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract file name from the origin column\n",
    "imageDF = imageDF.withColumn(\"file_name\", F.regexp_extract(\"image.origin\", r\"([^/]+)\\.png$\", 1))\n",
    "\n",
    "combinedDF = imageDF.join(jsonDF, imageDF.file_name == jsonDF.hexcode, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c26825-a595-464d-80f4-6818e55eec22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[image: struct<origin:string,height:int,width:int,nChannels:int,mode:int,data:binary>, hex_pixels: array<string>, color_names: array<string>, file_name: string, annotation: string, emoji: string, group: string, hexcode: string, openmoji_author: string, openmoji_date: string, openmoji_tags: string, order: bigint, skintone: string, skintone_base_emoji: string, skintone_base_hexcode: string, skintone_combination: string, subgroups: string, tags: string, unicode: string, colored_pixel_count: int, total_pixel_count: int, colored_to_transparent_ratio: double]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinedDF = combinedDF.withColumn(\n",
    "    \"colored_pixel_count\", \n",
    "    F.size(F.filter(F.col('color_names'), lambda color_name: color_name != 'None'))\n",
    ")\n",
    "\n",
    "combinedDF = combinedDF.withColumn(\"total_pixel_count\", F.size(F.col(\"hex_pixels\")))\n",
    "\n",
    "combinedDF = combinedDF.withColumn(\n",
    "    \"colored_to_transparent_ratio\", \n",
    "    F.col(\"colored_pixel_count\") / F.col(\"total_pixel_count\")\n",
    ")\n",
    "\n",
    "combinedDF.cache() #since this is reused in the next cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa08b5c-6884-40a3-a86c-c56ef5b993d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The annotations for the four images with the most colored non-transparent pixels:\n- flag: Finland\n- bowling\n- bullseye\n- volleyball\n============================================================\nThe annotations for the five images having the lowest ratio of colored vs. transparent pixels:\n- seedling\n- magic wand\n- herb\n- lizard\n- fireworks\n"
     ]
    }
   ],
   "source": [
    "# The annotations for the four images with the most colored non-transparent pixels\n",
    "mostColoredPixelsDF = combinedDF.orderBy(F.col(\"colored_pixel_count\").desc()).limit(4)\n",
    "mostColoredPixels: List[str] =  mostColoredPixelsDF.select(\"annotation\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "print(\"The annotations for the four images with the most colored non-transparent pixels:\")\n",
    "for image in mostColoredPixels:\n",
    "    print(f\"- {image}\")\n",
    "print(\"============================================================\")\n",
    "\n",
    "# The annotations for the five images having the lowest ratio of colored vs. transparent pixels\n",
    "leastColoredPixels_df = combinedDF.orderBy(F.col(\"colored_to_transparent_ratio\").asc()).limit(5)\n",
    "leastColoredPixels: List[str] = leastColoredPixels_df.select(\"annotation\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "print(\"The annotations for the five images having the lowest ratio of colored vs. transparent pixels:\")\n",
    "for image in leastColoredPixels:\n",
    "    print(f\"- {image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d958e11-e7bb-4274-a124-5f5a1d121fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The colors and their percentage shares in the image for the Finnish flag:\n- color: White, share: 56.47\n- color: Blue, share: 27.15\n- color: Black, share: 15.53\n============================================================\nThe number of images that have, Blue-Yellow-Black, as the most common colors: 6\n"
     ]
    }
   ],
   "source": [
    "finnishFlagDF = combinedDF.filter(combinedDF.annotation == \"flag: Finland\")  \n",
    "finnishFlagColorsDF = finnishFlagDF.select(\"color_names\").collect()\n",
    "\n",
    "# The three most common colors in the Finnish flag image:\n",
    "finnishFlagColors: List[str] = [color for row in finnishFlagColorsDF for color in row[\"color_names\"]]\n",
    "finnishFlagColors = [color for color in finnishFlagColors if color != \"None\"]\n",
    "\n",
    "colorCounts = Counter(finnishFlagColors)\n",
    "\n",
    "mostcommonColors = colorCounts.most_common(3)\n",
    "finnishFColors = [color for color, _ in mostcommonColors]\n",
    "\n",
    "totalPixels = len(finnishFlagColors)\n",
    "\n",
    "# The percentages of the colored pixels for each common color in the Finnish flag image:\n",
    "finnishColorShares: List[float] =  [round(count / totalPixels * 100, 2) for _, count in mostcommonColors]\n",
    "\n",
    "\n",
    "print(\"The colors and their percentage shares in the image for the Finnish flag:\")\n",
    "for color, share in zip(finnishFColors, finnishColorShares):\n",
    "    print(f\"- color: {color}, share: {share}\")\n",
    "print(\"============================================================\")\n",
    "\n",
    "\n",
    "def getmostCommonColors(color_names: List[str]) -> List[str]:\n",
    "    filteredColors = [color for color in color_names if color != \"None\"]\n",
    "    color_counts = Counter(filteredColors)\n",
    "    mostcommonColors = [color for color, _ in color_counts.most_common(3)]\n",
    "    return mostcommonColors\n",
    "\n",
    "get_most_common_colors_udf = F.udf(getmostCommonColors, ArrayType(StringType()))\n",
    "\n",
    "combinedDF = combinedDF.withColumn(\"most_common_colors\", get_most_common_colors_udf(F.col(\"color_names\")))\n",
    "\n",
    "# The number of images that have their most common three colors as, Blue-Yellow-Black, in that exact order:\n",
    "blueYellowBlackCount: int = combinedDF.filter(\n",
    "    (F.col(\"most_common_colors\")[0] == \"Blue\") & \n",
    "    (F.col(\"most_common_colors\")[1] == \"Yellow\") & \n",
    "    (F.col(\"most_common_colors\")[2] == \"Black\")\n",
    ").count()\n",
    "\n",
    "print(f\"The number of images that have, Blue-Yellow-Black, as the most common colors: {blueYellowBlackCount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81295faa-e689-47e7-a946-282ae7e531f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "activitiesDF = combinedDF.filter(combinedDF.group == \"activities\")\n",
    "\n",
    "\n",
    "def countredPixels(color_names: List[str]) -> int:\n",
    "    return color_names.count(\"Red\")\n",
    "\n",
    "countredpixelsUdf = F.udf(countredPixels, IntegerType())\n",
    "\n",
    "activitiesDF = activitiesDF.withColumn(\"red_pixel_count\", countredpixelsUdf(F.col(\"color_names\")))\n",
    "\n",
    "topredimagesDF = activitiesDF.orderBy(F.col(\"red_pixel_count\").desc()).limit(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6322b359-f55f-4835-b44e-6705b99863c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The annotations and red pixel counts for the five images with the most red pixels among the image group 'activities':\n- red envelope (red pixels: 1765)\n- admission tickets (red pixels: 1191)\n- flower playing cards (red pixels: 892)\n- reminder ribbon (red pixels: 764)\n- balloon (red pixels: 537)\n"
     ]
    }
   ],
   "source": [
    "# The annotations for the five images with the most red pixels among the image group activities:\n",
    "redImageNames: List[str] = topredimagesDF.select(\"annotation\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# The number of red pixels in the five images with the most red pixels among the image group activities:\n",
    "redPixelAmounts: List[int] = topredimagesDF.select(\"red_pixel_count\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "print(\"The annotations and red pixel counts for the five images with the most red pixels among the image group 'activities':\")\n",
    "for color, pixel_count in zip(redImageNames, redPixelAmounts):\n",
    "    print(f\"- {color} (red pixels: {pixel_count})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d994c5ad-8da5-42e9-a1d4-542a7186f859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Advanced Task 4 - Machine learning tasks (2 points)\n",
    "\n",
    "This advanced task involves experimenting with the classifiers provided by the Spark machine learning library. \n",
    "\n",
    "#### Data description\n",
    "\n",
    "The dataset contains time series data from a period of 13 months (from the beginning of May 2023 to the end of May 2024). Each row contains the average of the measured values for a single minute. The following columns are included in the data:\n",
    "\n",
    "| column name        | column type   | description |\n",
    "| ------------------ | ------------- | ----------- |\n",
    "| time               | long          | The UNIX timestamp in second precision |\n",
    "| temperature        | double        | The temperature measured by the weather station on top of Sähkötalo (`°C`) |\n",
    "| humidity           | double        | The humidity measured by the weather station on top of Sähkötalo (`%`) |\n",
    "| wind_speed         | double        | The wind speed measured by the weather station on top of Sähkötalo (`m/s`) |\n",
    "| power_tenants      | double        | The total combined electricity power used by the tenants on Kampusareena (`W`) |\n",
    "| power_maintenance  | double        | The total combined electricity power used by the building maintenance systems on Kampusareena (`W`) |\n",
    "| power_solar_panels | double        | The total electricity power produced by the solar panels on Kampusareena (`W`) |\n",
    "| electricity_price  | double        | The market price for electricity in Finland (`€/MWh`) |\n",
    "\n",
    "There are some missing values that need to be removed before using the data for training or testing. However, only the minimal amount of rows should be removed for each test case.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "- The main task is to train and test a machine learning model with [Random forest classifier](https://spark.apache.org/docs/3.5.0/ml-classification-regression.html#random-forests) in six different cases:\n",
    "    - Predict the month (1-12) using the three weather measurements (temperature, humidity, and wind speed) as input\n",
    "    - Predict the month (1-12) using the three power measurements (tenants, maintenance, and solar panels) as input\n",
    "    - Predict the month (1-12) using all seven measurements (weather values, power values, and price) as input\n",
    "    - Predict the hour of the day (0-23) using the three weather measurements (temperature, humidity, and wind speed) as input\n",
    "    - Predict the hour of the day (0-23) using the three power measurements (tenants, maintenance, and solar panels) as input\n",
    "    - Predict the hour of the day (0-23) using all seven measurements (weather values, power values, and price) as input\n",
    "- For each of the six case you are asked to:\n",
    "    1. Clean the source dataset from rows with missing values.\n",
    "    2. Split the dataset into training and test parts.\n",
    "    3. Train the ML model using a Random forest classifier with case-specific input and prediction.\n",
    "    4. Evaluate the accuracy of the model with Spark built-in multiclass classification evaluator.\n",
    "    5. Further evaluate the accuracy of the model with a custom build evaluator which should do the following:\n",
    "        - calculate the percentage of correct predictions\n",
    "            - this should correspond to the accuracy value from the built-in accuracy evaluator\n",
    "        - calculate the percentage of predictions that were at most one away from the correct predictions taking into account the cyclic nature of the month and hour values:\n",
    "            - if the correct month value was `5`, then acceptable predictions would be `4`, `5`, or `6`\n",
    "            - if the correct month value was `1`, then acceptable predictions would be `12`, `1`, or `2`\n",
    "            - if the correct month value was `12`, then acceptable predictions would be `11`, `12`, or `1`\n",
    "        - calculate the percentage of predictions that were at most two away from the correct predictions taking into account the cyclic nature of the month and hour values:\n",
    "            - if the correct month value was `5`, then acceptable predictions would be from `3` to `7`\n",
    "            - if the correct month value was `1`, then acceptable predictions would be from `11` to `12` and from `1` to `3`\n",
    "            - if the correct month value was `12`, then acceptable predictions would be from `10` to `12` and from `1` to `2`\n",
    "        - calculate the average probability the model predicts for the correct value\n",
    "            - the probabilities for a single prediction can be found from the `probability` column after the predictions have been made with the model\n",
    "- As the final part of this advanced task, you are asked to do the same experiments (training+evaluation) with two further cases of your own choosing:\n",
    "    - you can decide on the input columns yourself\n",
    "    - you can decide the predicted attribute yourself\n",
    "    - you can try some other classifier other than the random forest one if you want\n",
    "\n",
    "In all cases you are free to choose the training parameters as you wish.<br>\n",
    "Note that it is advisable that while you are building your task code to only use a portion of the full 13-month dataset in the initial experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93dd482c-4355-4fed-9a51-48a45b91cdbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "procemDF = spark.read.parquet(\"abfss://shared@tunics320f2024gen2.dfs.core.windows.net/assignment/energy/procem_13m.parquet\")\n",
    "procemDF = procemDF.withColumn(\"month\", F.from_unixtime(F.col(\"time\"), \"M\").cast(\"int\")) \\\n",
    "                           .withColumn(\"hour\", F.from_unixtime(F.col(\"time\"), \"H\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c50d40d-26e4-4f6b-ab46-b6d35a8caef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def trainandTest(data, input_features, label_column, classifier_type=\"RandomForest\"):\n",
    "\n",
    "    # removing missing values\n",
    "    cleanedData = data.dropna(subset=input_features + [label_column])\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")\n",
    "    cleanedData = assembler.transform(cleanedData)\n",
    "    \n",
    "    # Split the dataset\n",
    "    trainDF, testDF = cleanedData.randomSplit([0.8, 0.2], seed=1)\n",
    "        \n",
    "    classifiers = {\n",
    "        \"RandomForest\": RandomForestClassifier(labelCol=label_column, featuresCol=\"features\", probabilityCol=\"probabilities\"),\n",
    "        \"LogisticRegression\": LogisticRegression(labelCol=label_column, featuresCol=\"features\", probabilityCol=\"probabilities\")\n",
    "    }\n",
    "    \n",
    "    classifier = classifiers.get(classifier_type)\n",
    "    if not classifier:\n",
    "        raise ValueError(\"Classifier not supported\")\n",
    "    \n",
    "    # Train the model\n",
    "    model = classifier.fit(trainDF)\n",
    "    \n",
    "    # Testing\n",
    "    predictions = model.transform(testDF)\n",
    "    \n",
    "    # accuracy\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=label_column, metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    \n",
    "    totalPredictions = predictions.count()\n",
    "\n",
    "    # correct predictions\n",
    "    correct_predictions = predictions.filter(F.col(label_column) == F.col(\"prediction\")).count()\n",
    "    correct_percentage = round((correct_predictions / totalPredictions) * 100, 2)\n",
    "    \n",
    "    # within one step\n",
    "    withinonePredictions = predictions.filter(\n",
    "        (F.abs(F.col(label_column) - F.col(\"prediction\")) <= 1) |\n",
    "        (F.abs(F.col(label_column) - F.col(\"prediction\")) == 11)  # for(1-12)\n",
    "    ).count()\n",
    "    withinonePredictions = round((withinonePredictions / totalPredictions) * 100, 2) \n",
    "    \n",
    "    #within two steps\n",
    "    withintwoPredictions = predictions.filter(\n",
    "        (F.abs(F.col(label_column) - F.col(\"prediction\")) <= 2) |\n",
    "        (F.abs(F.col(label_column) - F.col(\"prediction\")) >= 10)  # for(1-12)\n",
    "    ).count()\n",
    "    withintwoPredictions = round((withintwoPredictions / totalPredictions) * 100, 2)\n",
    "    \n",
    "\n",
    "    #average correct probability\n",
    "    predictions = predictions.withColumn(\"prob_array\", vector_to_array(F.col(\"probabilities\")))\n",
    "    predictions = predictions.withColumn(\n",
    "        \"correct_prob\",\n",
    "        F.element_at(F.col(\"prob_array\"), (F.col(\"prediction\") + 1).cast(\"int\"))\n",
    "    )\n",
    "    \n",
    "    correctPredictions_df = predictions.filter(F.col(label_column) == F.col(\"prediction\"))\n",
    "    \n",
    "    avgProb = round(correctPredictions_df.agg(F.avg(\"correct_prob\").alias(\"avg_prob\")).collect()[0][0], 3)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy, \n",
    "        \"correct_percentage\": correct_percentage,\n",
    "        \"within_one_percentage\": withinonePredictions,\n",
    "        \"within_two_percentage\": withintwoPredictions,\n",
    "        \"avg_prob\": avgProb\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4014b073-70b2-4d72-8915-3437e6d23d86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a 'RandomForest' model to predict 'month' based on input 'temperature, humidity, wind_speed'.\nThe accuracy of the model is 0.45197\nTraining a 'RandomForest' model to predict 'hour' based on input 'temperature, humidity, wind_speed'.\nThe accuracy of the model is 0.07572\nTraining a 'RandomForest' model to predict 'month' based on input 'power_tenants, power_maintenance, power_solar_panels'.\nThe accuracy of the model is 0.28023\nTraining a 'RandomForest' model to predict 'hour' based on input 'power_tenants, power_maintenance, power_solar_panels'.\nThe accuracy of the model is 0.22178\nTraining a 'RandomForest' model to predict 'month' based on input 'temperature, humidity, wind_speed, power_tenants, power_maintenance, power_solar_panels, electricity_price'.\nThe accuracy of the model is 0.51026\nTraining a 'RandomForest' model to predict 'hour' based on input 'temperature, humidity, wind_speed, power_tenants, power_maintenance, power_solar_panels, electricity_price'.\nThe accuracy of the model is 0.25110\n+------------+----------------------------------------------------------------------------------------------------------+-----+-------+----------+----------+--------+\n|classifier  |input                                                                                                     |label|correct|within_one|within_two|avg_prob|\n+------------+----------------------------------------------------------------------------------------------------------+-----+-------+----------+----------+--------+\n|RandomForest|temperature, humidity, wind_speed, power_tenants, power_maintenance, power_solar_panels, electricity_price|month|51.03  |77.79     |85.83     |0.386   |\n|RandomForest|temperature, humidity, wind_speed                                                                         |month|45.2   |73.34     |83.27     |0.405   |\n|RandomForest|power_tenants, power_maintenance, power_solar_panels                                                      |month|28.02  |53.44     |69.94     |0.26    |\n|RandomForest|temperature, humidity, wind_speed, power_tenants, power_maintenance, power_solar_panels, electricity_price|hour |25.11  |52.6      |81.05     |0.173   |\n|RandomForest|power_tenants, power_maintenance, power_solar_panels                                                      |hour |22.18  |48.93     |79.52     |0.207   |\n|RandomForest|temperature, humidity, wind_speed                                                                         |hour |7.57   |23.04     |65.55     |0.076   |\n+------------+----------------------------------------------------------------------------------------------------------+-----+-------+----------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "cases = [\n",
    "    ([\"temperature\", \"humidity\", \"wind_speed\"], \"month\", \"RandomForest\"),\n",
    "    ([\"temperature\", \"humidity\", \"wind_speed\"], \"hour\", \"RandomForest\"),\n",
    "    ([\"power_tenants\", \"power_maintenance\", \"power_solar_panels\"], \"month\", \"RandomForest\"),\n",
    "    ([\"power_tenants\", \"power_maintenance\", \"power_solar_panels\"], \"hour\", \"RandomForest\"),\n",
    "    ([\"temperature\", \"humidity\", \"wind_speed\", \"power_tenants\", \"power_maintenance\", \"power_solar_panels\", \"electricity_price\"], \"month\", \"RandomForest\"),\n",
    "    ([\"temperature\", \"humidity\", \"wind_speed\", \"power_tenants\", \"power_maintenance\", \"power_solar_panels\", \"electricity_price\"], \"hour\", \"RandomForest\")\n",
    "]\n",
    "\n",
    "# Process cases and store results in a DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"classifier\", StringType(), True),\n",
    "    StructField(\"input\", StringType(), True),\n",
    "    StructField(\"label\", StringType(), True),\n",
    "    StructField(\"correct\", DoubleType(), True),\n",
    "    StructField(\"within_one\", DoubleType(), True),\n",
    "    StructField(\"within_two\", DoubleType(), True),\n",
    "     StructField(\"avg_prob\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "resultsData = []\n",
    "for input_features, label_column, classifier_type in cases:\n",
    "    # Train and evaluate the model\n",
    "    result = trainandTest(procemDF, input_features, label_column)\n",
    "    \n",
    "    input_features_str = \", \".join(input_features)\n",
    "    model_name = classifier_type\n",
    "    accuracy = result[\"accuracy\"]\n",
    "\n",
    "    print(f\"Training a '{model_name}' model to predict '{label_column}' based on input '{input_features_str}'.\")\n",
    "    print(f\"The accuracy of the model is {accuracy:.5f}\")\n",
    "\n",
    "    # Results\n",
    "    resultsData.append((\n",
    "        model_name,\n",
    "        input_features_str,\n",
    "        label_column,\n",
    "        result[\"correct_percentage\"],\n",
    "        result[\"within_one_percentage\"],\n",
    "        result[\"within_two_percentage\"],\n",
    "        result[\"avg_prob\"] \n",
    "        \n",
    "    ))\n",
    "\n",
    "resultsDF = spark.createDataFrame(resultsData, schema=schema)\n",
    "resultsDF = resultsDF.orderBy(F.col(\"correct\"), ascending=False)\n",
    "resultsDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53d5fab-ed9d-41ea-a3a7-ed707fcc502b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a 'RandomForest' model to predict 'month' based on input 'power_tenants, power_maintenance, electricity_price'.\nThe accuracy of the model is 0.26905\nTraining a 'LogisticRegression' model to predict 'month' based on input 'temperature, humidity, wind_speed, power_tenants, power_maintenance, power_solar_panels, electricity_price'.\nThe accuracy of the model is 0.45045\n+------------------+----------------------------------------------------------------------------------------------------------+-----+-------+----------+----------+--------+\n|classifier        |input                                                                                                     |label|correct|within_one|within_two|avg_prob|\n+------------------+----------------------------------------------------------------------------------------------------------+-----+-------+----------+----------+--------+\n|LogisticRegression|temperature, humidity, wind_speed, power_tenants, power_maintenance, power_solar_panels, electricity_price|month|45.05  |72.63     |83.32     |0.474   |\n|RandomForest      |power_tenants, power_maintenance, electricity_price                                                       |month|26.9   |47.54     |59.38     |0.272   |\n+------------------+----------------------------------------------------------------------------------------------------------+-----+-------+----------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# two further cases first with \"power_tenants\", \"power_maintenance\", \"electricity_price\" variables with Randomforest\n",
    "# second one with \"temperature\", \"humidity\", \"wind_speed\", \"power_tenants\", \"power_maintenance\", \"power_solar_panels\", \"electricity_price\" variables with Logistic regression\n",
    "cases = [\n",
    "    ([\"power_tenants\", \"power_maintenance\", \"electricity_price\"], \"month\", \"RandomForest\"),  # Random Forest Classifier\n",
    "    ([\"temperature\", \"humidity\", \"wind_speed\", \"power_tenants\", \"power_maintenance\", \"power_solar_panels\", \"electricity_price\"], \"month\", \"LogisticRegression\")  # Logistic Regression Classifier\n",
    "]\n",
    "\n",
    "\n",
    "resultsData = []\n",
    "for input_features, label_column, classifier_type in cases:\n",
    "\n",
    "    result = trainandTest(procemDF, input_features, label_column, classifier_type)\n",
    "    \n",
    "    input_features_str = \", \".join(input_features)\n",
    "    model_name = classifier_type\n",
    "    accuracy = result[\"accuracy\"]\n",
    "    \n",
    "    print(f\"Training a '{model_name}' model to predict '{label_column}' based on input '{input_features_str}'.\")\n",
    "    print(f\"The accuracy of the model is {accuracy:.5f}\")\n",
    "\n",
    "    # Results\n",
    "    resultsData.append((\n",
    "        model_name,\n",
    "        input_features_str,\n",
    "        label_column,\n",
    "        result[\"correct_percentage\"],\n",
    "        result[\"within_one_percentage\"],\n",
    "        result[\"within_two_percentage\"],\n",
    "        result[\"avg_prob\"]\n",
    "    ))\n",
    "\n",
    "resultsDF = spark.createDataFrame(resultsData, schema=schema)\n",
    "resultsDF = resultsDF.orderBy(F.col(\"correct\"), ascending=False)\n",
    "resultsDF.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "(Clone) Assignment-python-completed",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}